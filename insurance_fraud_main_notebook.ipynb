{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tajuar-akash-hub/test_repo_for_colaboration/blob/main/insurance_fraud_main_notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CX2-WCpBovkR"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/tajuar-akash-hub/Datasets.git"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "insurance_fraud=pd.read_csv('/content/Datasets/Health Insurance Fraud Claims csv file.csv')\n",
        "\n",
        "insurance_fraud2 = insurance_fraud  #copy of the main"
      ],
      "metadata": {
        "id": "SzGrtAtCUqhx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "insurance_fraud.head(1)"
      ],
      "metadata": {
        "id": "CktcrKGRLbMW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# info about the datasets(datatype,null-non_null)"
      ],
      "metadata": {
        "id": "etlAOOEupCKt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "insurance_fraud.info()"
      ],
      "metadata": {
        "id": "a0fo0MWtkoJY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Create A copy"
      ],
      "metadata": {
        "id": "0_I5tfCvx9C1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Check the missing value(null values)"
      ],
      "metadata": {
        "id": "g7dVPbkdpGmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "insurance_fraud.isnull().sum()"
      ],
      "metadata": {
        "id": "yRCljYjuot--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "insurance_fraud.head(2)"
      ],
      "metadata": {
        "id": "KXyHJds6_ShI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Take a description about datasets(min,max,mean)"
      ],
      "metadata": {
        "id": "pDcCgMK7SHkl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "insurance_fraud.describe()"
      ],
      "metadata": {
        "id": "PxRQjlnjpOD9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check number of applicant whose age is less than 18"
      ],
      "metadata": {
        "id": "Fdlae_GTdbyF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "age_under_18 = insurance_fraud[insurance_fraud['PatientAge']< 18]\n",
        "len(age_under_18)\n"
      ],
      "metadata": {
        "id": "EDzUra2JdI3g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#remove all the rows age_under_18"
      ],
      "metadata": {
        "id": "AGkqdeQLdSbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "insurance_fraud=insurance_fraud.drop(age_under_18.index)\n",
        "\n",
        "insurance_fraud.shape"
      ],
      "metadata": {
        "id": "7t4W5UkDdK-O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# check outliers in incomes and age\n",
        "\n",
        "### 2.1 using Z-scores"
      ],
      "metadata": {
        "id": "SyK608L1rh7w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# using z-scores few more outliers algorithms use\n",
        "def detect_outliers(data,x):\n",
        "  outliers=[]\n",
        "  thresold=1.5  #why not 3?\n",
        "  mean=np.mean(data[x])\n",
        "  std=np.std(data[x])\n",
        "  for i in data[x]:\n",
        "    z_score=(i-mean)/std\n",
        "    if np.abs(z_score) > thresold:\n",
        "      outliers.append(i)\n",
        "  return outliers\n"
      ],
      "metadata": {
        "id": "7rdfToIut7Qn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#outliersb using z-scores with threshold 1.5\n",
        "print(f\"outliers in PatientIncome : {len(detect_outliers(insurance_fraud,'PatientIncome'))}\")\n",
        "print(f\"outliers in PatientAge : {len(detect_outliers(insurance_fraud,'PatientAge'))}\")\n",
        "print(f\"outliers in claimamount : {len(detect_outliers(insurance_fraud,'ClaimAmount'))}\")\n"
      ],
      "metadata": {
        "id": "KKjR1FRexsjI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###2.2 using IQR"
      ],
      "metadata": {
        "id": "yTtXD5Ey0J8y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def iqr_outliers(data,x):\n",
        "  Q1=np.percentile(data[x],25)\n",
        "  Q3=np.percentile(data[x],75)\n",
        "  IQR=Q3-Q1\n",
        "  lower_bound=Q1 - (1.5 * IQR)\n",
        "  upper_bound=Q1 + (1.5 * IQR)\n",
        "  outliers=[]\n",
        "\n",
        "  for i in data[x]:\n",
        "    if i< lower_bound or i > upper_bound:\n",
        "      outliers.append(i)\n",
        "  return outliers\n",
        "\n"
      ],
      "metadata": {
        "id": "JoKJ6pLZ0NZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## outliers in PatientIncome\n",
        "income_outliers=len(iqr_outliers(insurance_fraud,\"PatientIncome\"))\n",
        "# outliers in ClaimAmount\n",
        "claim_outliers=len(iqr_outliers(insurance_fraud,\"ClaimAmount\"))\n",
        "age_outliers=len(iqr_outliers(insurance_fraud,\"PatientAge\"))\n",
        "\n",
        "print(f\"outliers in PatientIncome : {income_outliers}\")\n",
        "print(f\"outliers in claimamount : {claim_outliers}\")\n",
        "print(f\"outliers in PatientAge : {age_outliers}\")\n"
      ],
      "metadata": {
        "id": "FWgZJP9Ayt57"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "sns.set_theme(color_codes=\"red\")\n",
        "sns.displot(data=insurance_fraud[\"PatientAge\"]).set(title=\"Distribution of age\",xlabel=\"Ages\")"
      ],
      "metadata": {
        "id": "MWjmWLAhrhj5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#remove unwanted columns from the datasets\n"
      ],
      "metadata": {
        "id": "H164E_dXU4Qc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "insurance_fraud = insurance_fraud[[\"ClaimDate\",\"ClaimAmount\",\"PatientAge\",\"PatientGender\",\"ProviderSpecialty\",\"ClaimStatus\",\"PatientIncome\",\"PatientMaritalStatus\",\"PatientEmploymentStatus\",\"ProviderLocation\",\"ClaimType\",\"ClaimSubmissionMethod\",\"ClaimLegitimacy\"]]\n",
        "\n",
        "print(f\"Shape of the datasets: {insurance_fraud.shape}\")\n",
        "\n",
        "insurance_fraud.head(1)\n",
        "\n"
      ],
      "metadata": {
        "id": "2OUU9FwhVLwF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "insurance_fraud[['PatientAge']]"
      ],
      "metadata": {
        "id": "hR9Hiexn60cx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# all coulmns unique values"
      ],
      "metadata": {
        "id": "aklxchjp5Kys"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_unique=insurance_fraud.nunique()\n",
        "all_unique_df = pd.DataFrame(all_unique, columns=['Unique Value Count'])\n",
        "all_unique_df"
      ],
      "metadata": {
        "id": "W5emFM5puLRH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Check duplicated values"
      ],
      "metadata": {
        "id": "qIoMLJ0bfpOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duplicates = insurance_fraud.duplicated()\n",
        "flag=0\n",
        "for duplicate in duplicates:\n",
        "  if duplicate == True:\n",
        "    flag=1\n",
        "    print(f\"duplicate values {duplicate}\")\n",
        "if flag == 0:\n",
        "  print(\"No duplicates row\")\n",
        "\n"
      ],
      "metadata": {
        "id": "GU953VYcfs1G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Copy of Original data"
      ],
      "metadata": {
        "id": "erGxX0F53uYE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "copy_insurance_fraud=insurance_fraud.copy()"
      ],
      "metadata": {
        "id": "3_0FjkYT30FD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Encoding Categorical Data"
      ],
      "metadata": {
        "id": "FymiD4k5jLPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder,OrdinalEncoder\n",
        "le=LabelEncoder()"
      ],
      "metadata": {
        "id": "JpVrXU_6jN1B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Use Ordinal encoders to encode ClaimStatus"
      ],
      "metadata": {
        "id": "ucZU-J_knfZt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ordinal_categories=[\"Denied\",\"Pending\",\"Approved\"]\n",
        "\n",
        "ordinal = OrdinalEncoder(categories=[ordinal_categories])\n",
        "\n",
        "insurance_fraud[\"ClaimStatus\"]=ordinal.fit_transform(insurance_fraud[[\"ClaimStatus\"]])\n",
        "\n",
        "insurance_fraud.head()"
      ],
      "metadata": {
        "id": "vfWwYQ1NnnZ8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "categorical_columns = [\"PatientGender\",\n",
        "                     \"ClaimLegitimacy\",\n",
        "                     \"ProviderSpecialty\",\n",
        "                     \"PatientMaritalStatus\",\n",
        "                     \"PatientEmploymentStatus\",\n",
        "                     \"ProviderLocation\",\n",
        "                     \"ClaimType\",\n",
        "                     \"ClaimSubmissionMethod\"\n",
        "                     ]\n",
        "\n",
        "for col in categorical_columns:\n",
        "  insurance_fraud[col]=le.fit_transform(insurance_fraud[col])\n",
        "\n",
        "insurance_fraud.head()"
      ],
      "metadata": {
        "id": "KzVfaMwujOLX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "insurance_fraud.info()"
      ],
      "metadata": {
        "id": "84FeikibvjPY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "Legit_Claim = insurance_fraud[\"ClaimLegitimacy\"].value_counts()\n",
        "\n",
        "print(f\" Value counts of Legit_Claim: {Legit_Claim}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "h5WDhaMsZ9JU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# OverSampling to Spread the target data equal class"
      ],
      "metadata": {
        "id": "ZICKjH9hyxrF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import SMOTE\n"
      ],
      "metadata": {
        "id": "W43AKnCizBEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Convert 'ClaimDate' to ordinal\n",
        "\n",
        "insurance_fraud['ClaimDate'] = pd.to_datetime(insurance_fraud['ClaimDate']).apply(lambda date: date.toordinal())\n",
        "\n",
        " #  After converting to datetime, this applies a\n",
        " #lambda function to each date in the ClaimDate\n",
        "\n",
        "\n",
        "\n",
        "#feature and target split\n",
        "\n",
        "X = insurance_fraud.drop('ClaimLegitimacy',axis=1)  #feature\n",
        "\n",
        "y = insurance_fraud['ClaimLegitimacy']  #target\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "X_resampled , y_resampled = smote.fit_resample(X,y)\n",
        "\n",
        "oversampled_df = pd.DataFrame(X_resampled , columns = X.columns)\n",
        "\n",
        "oversampled_df['ClaimLegitimacy'] = y_resampled   #adding a new column n to oversampled_df name 'class' which holds the target"
      ],
      "metadata": {
        "id": "6PN0UAAJzGbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Shape of new Oversampled_df datasets"
      ],
      "metadata": {
        "id": "cDn8pzJU0Z92"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "value_counts = oversampled_df.groupby('ClaimLegitimacy').size().reset_index(name = 'Count')\n",
        "\n",
        "print(value_counts)\n"
      ],
      "metadata": {
        "id": "Dr5L7l3z0MHC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oversampled_df.shape"
      ],
      "metadata": {
        "id": "_2J3XTfGhO_t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# EDA Section"
      ],
      "metadata": {
        "id": "rqcOBlKl1Tof"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Grouping by Gender and Fraud Status to sum the Claim Amounts\n",
        "grouped_data = copy_insurance_fraud.groupby(['PatientMaritalStatus', 'ClaimLegitimacy'])['ClaimAmount'].sum().reset_index()\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Barplot to show Total Claim Amount vs Fraud for Male and Female\n",
        "sns.barplot(x='PatientMaritalStatus', y='ClaimAmount', hue='ClaimLegitimacy', data=grouped_data)\n",
        "\n",
        "# Labels and Title\n",
        "plt.title('Total Claim Amount vs Fraud Status by Gender', fontsize=16)\n",
        "plt.xlabel('Gender', fontsize=14)\n",
        "plt.ylabel('Total Claim Amount', fontsize=14)\n",
        "\n",
        "# Show plot\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "LK9R4p301TC5",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fradulent activity by Age Range"
      ],
      "metadata": {
        "id": "Aiox5Qp54kAq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "copy_insurance_fraud['ClaimLegitimacy'].value_counts()"
      ],
      "metadata": {
        "id": "li5M2QUpkX7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fraud activaity by age"
      ],
      "metadata": {
        "id": "cGvFVJCFw01i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Filter only fraudulent claims (ClaimLegitimacy = 0 for fraudulent)\n",
        "\n",
        "fraudulent_claims =copy_insurance_fraud[copy_insurance_fraud['ClaimLegitimacy'] == \"Fraud\"]\n",
        "\n",
        "# Define age ranges (bins) for grouping\n",
        "\n",
        "age_bins = [0, 20, 40, 60, 80, 100]\n",
        "age_labels = ['0-20', '21-40', '41-60', '61-80', '81-100']\n",
        "\n",
        "# Create a new column 'AgeRange' based on the bins\n",
        "\n",
        "fraudulent_claims['AgeRange'] = pd.cut(fraudulent_claims['PatientAge'], bins=age_bins, labels=age_labels)\n",
        "\n",
        "# Group by AgeRange and count the number of fraudulent claims\n",
        "age_fraud_count = fraudulent_claims.groupby('AgeRange').size().reset_index(name='FraudCount')\n",
        "\n",
        "# Plot the results\n",
        "plt.figure(figsize=(10, 6))\n",
        "sns.barplot(x='AgeRange', y='FraudCount', data=age_fraud_count, palette='coolwarm')\n",
        "\n",
        "plt.title('Fraudulent Activity by Age Range', fontsize=16)\n",
        "plt.xlabel('Age Range', fontsize=14)\n",
        "plt.ylabel('Number of Fraudulent Claims', fontsize=14)\n",
        "plt.xticks(fontsize=12)\n",
        "plt.yticks(fontsize=12)\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TzIJpthJ4mxA",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Gender distribution"
      ],
      "metadata": {
        "id": "7rBA7pRywwnB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "set\n",
        "plt.figure(figsize=(6,4))  # Larger figure size for better visibility\n",
        "\n",
        "# Calculate counts\n",
        "counts =insurance_fraud['PatientGender'].value_counts()\n",
        "\n",
        "# Plotting bar chart\n",
        "sns.barplot(x=counts.index, y=counts.values, palette='Set2')\n",
        "\n",
        "# Adding title and labels\n",
        "\n",
        "plt.title(\"Gender count\", fontsize=16)\n",
        "plt.xlabel(\"Gender\", fontsize=14)\n",
        "plt.ylabel('Count', fontsize=14)\n",
        "\n",
        "plt.xticks(rotation=5)  # Rotate x labels for better readability if needed\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "MxZQFaVvINW8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "insurance_fraud.info()"
      ],
      "metadata": {
        "id": "KOzxBtI2hDXb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copy_insurance_fraud[\"ClaimType\"].value_counts()"
      ],
      "metadata": {
        "id": "eYdqt4QOnC38"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns  # Added import for seaborn since you're using its color palette\n",
        "\n",
        "features=[\"PatientMaritalStatus\",\"PatientEmploymentStatus\",\"ClaimSubmissionMethod\",\"ClaimType\"]\n",
        "\n",
        "# Loop through each feature to generate a pie chart\n",
        "for f in features:\n",
        "    plt.figure(figsize=(6,4))  # Set the figure size to 10x8 inches for better visibility\n",
        "\n",
        "    # Calculate the value counts for the feature, normalize them to percentages\n",
        "    counts = copy_insurance_fraud[f].value_counts(normalize=True) * 100  # Multiplied by 100 to get percentages\n",
        "\n",
        "    # Plotting a pie chart with percentage labels on each slice\n",
        "    plt.pie(counts,\n",
        "            labels=counts.index,  # Label each slice with the unique values of the feature\n",
        "            autopct='%1.1f%%',  # Show percentage with 1 decimal place\n",
        "            startangle=140,  # Rotate the chart for a consistent starting point\n",
        "            colors=sns.color_palette('Set2'))  # Use a color palette from seaborn\n",
        "\n",
        "    # Add a title indicating which feature the pie chart represents\n",
        "    plt.title(f'{f} Distribution', fontsize=16)\n",
        "\n",
        "    # Ensures the pie chart is a perfect circle\n",
        "    plt.axis('equal')\n",
        "\n",
        "    # Display the pie chart\n",
        "    plt.show()\n",
        "    print(\"\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "BUwXz4wQxIH4",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "features=[\"PatientMaritalStatus\",\"PatientEmploymentStatus\",\"ClaimSubmissionMethod\",\"ClaimType\"]\n",
        "\n",
        "for f in features:\n",
        "  plt.figure(figsize=(6,4))  # Larger figure size for better visibility\n",
        "\n",
        "# Calculate counts\n",
        "counts = copy_insurance_fraud[f].value_counts()\n",
        "\n",
        "# Plotting bar chart\n",
        "sns.barplot(x=counts.index, y=counts.values, palette='Set2')\n",
        "\n",
        "# Adding title and labels\n",
        "\n",
        "plt.title(f'{f} Count', fontsize=16)\n",
        "plt.xlabel(f'{f}', fontsize=14)\n",
        "plt.ylabel('Count', fontsize=14)\n",
        "\n",
        "plt.xticks(rotation=45)  # Rotate x labels for better readability if needed\n",
        "\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "v6MgGUjXg15y",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create heatmap to show co-relations between features"
      ],
      "metadata": {
        "id": "KSpWrt9Lx2us"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#pearson corelation /heatmap\n",
        "sns.set(font_scale=2)\n",
        "plt.subplots(figsize=(30,30))\n",
        "heat_plot=sns.heatmap(insurance_fraud.corr(method='pearson'),annot=True,cmap='RdYlGn',annot_kws={'size':20})\n",
        "plt.yticks(fontsize=35)\n",
        "plt.xticks(fontsize=35)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "VQ_M8Hvrg40g",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Spliting (Taget and features variables)"
      ],
      "metadata": {
        "id": "vQKewOs7yeEC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#import libraries\n",
        "from sklearn.model_selection import train_test_split,KFold,cross_val_score\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ],
      "metadata": {
        "id": "_kZ1vHxN3dIZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "oversampled_df.head()"
      ],
      "metadata": {
        "id": "IQ70tqiWhuWJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Scaling Dataset"
      ],
      "metadata": {
        "id": "zx7wLnHOQZ5R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "min_max=MinMaxScaler()\n",
        "\n",
        "#train test split on oversampled_df\n",
        "\n",
        "X  = oversampled_df.drop('ClaimLegitimacy',axis=1)\n",
        "                                                                               #Changed_somethings here\n",
        "y = oversampled_df['ClaimLegitimacy']\n",
        "\n",
        "X_scaled = min_max.fit_transform(X)\n",
        "\n",
        "X_train,X_test,y_train,y_test=train_test_split(X_scaled,y,test_size=0.2,random_state=42)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6pSxi5e3yCnn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape,X_test.shape,y_train.shape,y_test.shape"
      ],
      "metadata": {
        "id": "BnXimrSO20C_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Start Applying ML Models ⬇\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Mm-stO4bA27u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# RandomForest Classifier\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "YdrjOTEEAj9n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "random_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# random_model.fit(X_train,y_train)\n",
        "\n",
        "# Train the model\n",
        "random_model.fit(X_train , y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = random_model.predict(X_test)\n",
        "\n",
        "# Evaluate model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "plt.figure(figsize=(7,5))\n",
        "sn.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')\n"
      ],
      "metadata": {
        "id": "TQYnW6Gj3BPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(random_model, X_train, y_train, cv=5)    #By Akash (experimental)\n",
        "\n",
        "print(f'Cross-validation scores: {scores}')\n",
        "print(f'Mean accuracy: {scores.mean() * 100:.2f}%')\n"
      ],
      "metadata": {
        "id": "mxm0-0HICC8P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support Vector Classifier"
      ],
      "metadata": {
        "id": "JP3oZuYMXJ2e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "svc = SVC()\n",
        "\n",
        "# Train the model\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svc.predict(X_test)\n",
        "\n",
        "# Evaluate model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "plt.figure(figsize=(5,5))\n",
        "sn.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')"
      ],
      "metadata": {
        "id": "eqvpytuYWcc9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN"
      ],
      "metadata": {
        "id": "JrLtZIV1bGMw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Train the model\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Evaluate model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')                                        #do we need to Scale our data for KNN ( by Akash) ?\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "plt.figure(figsize=(5,5))\n",
        "sn.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')"
      ],
      "metadata": {
        "id": "-MUO1PfTWrhW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression (By Akash)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "73jUcfelWDuE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(penalty = 'l2' , tol = 0.0001 , solver = 'liblinear',\n",
        "                           max_iter=100,multi_class='auto')\n",
        "\n",
        "#tol = tolerance , max_itr = max iteration\n",
        "\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "pred_values = model.predict(X_test)\n"
      ],
      "metadata": {
        "id": "6CAlmfqQWEEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(X_train,y_train)"
      ],
      "metadata": {
        "id": "q7hyEeaPWEHv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pred_values = model.predict(X_test)"
      ],
      "metadata": {
        "id": "xOdXNhMNcBex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Get predictions (class labels)\n",
        "pred_values = model.predict(X_test)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, pred_values)\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n"
      ],
      "metadata": {
        "id": "ubLRyunbZIBT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluation Metrices"
      ],
      "metadata": {
        "id": "oxolAVt6W6u0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score"
      ],
      "metadata": {
        "id": "QmmcFwJ6W38F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy = accuracy_score(y_test,pred_values)\n",
        "\n",
        "precision = precision_score(y_test,pred_values)\n",
        "\n",
        "recall = recall_score(y_test,pred_values)\n",
        "\n",
        "f1 = f1_score(y_test,pred_values)\n",
        "\n",
        "AUC = roc_auc_score(y_test,pred_values)"
      ],
      "metadata": {
        "id": "OoxAMKkdW3-6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"accuracy : \",accuracy*100)\n",
        "print(\"precision : \",precision*100)\n",
        "print(\"recall : \",recall*100)\n",
        "print(\"f1 score : \",f1*100)\n",
        "print(\"AUC Score : \",AUC*100)"
      ],
      "metadata": {
        "id": "src4_LkDXFJC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ROC CURVE"
      ],
      "metadata": {
        "id": "vuOc-FmXXWb8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Get predicted probabilities\n",
        "y_pred_prob = model.predict_proba(X_test)[:, 1]  # Probability for the positive class (1)\n",
        "\n",
        "# Calculate fpr and tpr for the ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.plot(fpr, tpr)\n",
        "plt.plot([0, 1], ls=\"--\")  # Random guess line\n",
        "\n",
        "plt.title(\"ROC Curve for Logistic Regression\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FhqXKU9xXYm6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes Classifier (By akash)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "PCQc2dMzNM1x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "AvegFW9ENPfe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Standard Scaler for Naive Bayes\n",
        "\n",
        "STD_Scaler = StandardScaler()\n",
        "STD_Scaled_X =  STD_Scaler.fit_transform(X)\n",
        "STD_Scaled_X"
      ],
      "metadata": {
        "id": "bfoZsvWlOJtM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#train-test split\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train,x_test,y_train,y_test = train_test_split(STD_Scaled_X,y,test_size=0.20)\n",
        "\n",
        "print(\"train data size (features):\",len(x_train))\n",
        "print(\"train data size (target):\",len(y_train))\n",
        "\n",
        "print(\"test data size(feature)\",len(x_test))\n",
        "print(\"test data size(target)\",len(y_test))\n",
        "\n"
      ],
      "metadata": {
        "id": "fRqZeotqOJwD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train)"
      ],
      "metadata": {
        "id": "JuNrdGSVOrQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#K-fold validation\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "NB_Classifier = GaussianNB(priors= None , var_smoothing =  1e-09 )    #1e-09 = 1*10^-9\n",
        "\n",
        "#we add very small value in var_smoothing to avoid 0/0 form\n",
        "\n",
        "# priors = prior probability  (see the documentation from sklearn)\n",
        "\n",
        "# var_smoothing = variance smothing where variance = sigma square\n",
        "\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "K_fold = KFold(10)\n",
        "\n",
        "accuracy = cross_val_score(NB_Classifier , X_train,y_train, cv =K_fold , scoring = 'accuracy' )\n",
        "\n",
        "precision = cross_val_score(NB_Classifier , X_train,y_train, cv =K_fold , scoring = 'precision' )\n",
        "\n",
        "recall = cross_val_score(NB_Classifier , X_train,y_train, cv =K_fold , scoring = 'recall' )\n",
        "\n",
        "f1_score = cross_val_score(NB_Classifier , X_train,y_train, cv =K_fold , scoring = 'f1' )\n",
        "\n",
        "AUC = cross_val_score(NB_Classifier , X_train,y_train, cv =K_fold , scoring = 'roc_auc' )"
      ],
      "metadata": {
        "id": "Igi610rlOrS2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy\n",
        "overall_accuracy = sum(accuracy) / len(accuracy)\n",
        "print(f'Overall Accuracy = {overall_accuracy*100:0.2f}%')\n",
        "\n",
        "\n",
        "recall\n",
        "overall_recall = sum(recall) / len(recall)\n",
        "print(f'overall_recall = {overall_recall*100:0.2f}%')\n",
        "\n",
        "\n",
        "f1_score\n",
        "\n",
        "overall_f1_score = sum(f1_score) / len(f1_score)\n",
        "print(f'overall_f1_score = {overall_f1_score*100:0.2f}%')\n",
        "\n",
        "\n",
        "overall_AUC = sum(AUC) / len(AUC)\n",
        "print(f'overall_AUC = {overall_AUC*100:0.2f}%')"
      ],
      "metadata": {
        "id": "ONrW3WNgOrX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying the Algorithm after k-fold(hyper parameter tuning phase)\n",
        "\n",
        "NB_Classifier = GaussianNB(priors= None , var_smoothing =  1e-09 )  #tune var_smoothing\n",
        "NB_Classifier.fit(X_train,y_train)\n",
        "\n",
        "#unseen data\n",
        "\n",
        "unseen_prediction = NB_Classifier.predict(X_test)\n"
      ],
      "metadata": {
        "id": "kGPceDMnPRNX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation metrices\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score,recall_score,f1_score ,roc_curve ,roc_auc_score\n",
        "\n",
        "accuracy = accuracy_score(unseen_prediction,y_test)\n",
        "\n",
        "precision = precision_score(unseen_prediction,y_test)\n",
        "\n",
        "recall = recall_score(unseen_prediction,y_test)\n",
        "\n",
        "f1 = f1_score(unseen_prediction,y_test)\n",
        "\n",
        "AUC = roc_auc_score(unseen_prediction,y_test)\n",
        "\n",
        "print(f'accuracy = {accuracy*100:0.2f}%')\n",
        "print(f'precision = {precision*100:0.2f}%')\n",
        "print(f'recall = {recall*100:0.2f}%')\n",
        "print(f'f1 = {f1*100:0.2f}%')\n"
      ],
      "metadata": {
        "id": "1TakC3nBPRQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Classifier"
      ],
      "metadata": {
        "id": "tcTNN_jyLKah"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_train)\n"
      ],
      "metadata": {
        "id": "ScnoEvgdZmW6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train)"
      ],
      "metadata": {
        "id": "oi44t52BZykF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "DTregressor=DecisionTreeRegressor(criterion='squared_error',\n",
        "                                  max_depth= 10,\n",
        "                                  min_samples_split= 2,\n",
        "                                  min_samples_leaf= 1,\n",
        "                                  max_features= None,\n",
        "                                  max_leaf_nodes= None,\n",
        "                                  min_impurity_decrease= 0.01\n",
        "                                  )"
      ],
      "metadata": {
        "id": "fn-5XlRp133J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "k_fold = KFold(10)\n",
        "\n",
        "results1 = cross_val_score(DTregressor, x, y, cv=k_fold , scoring='neg_mean_squared_error')\n",
        "results2 = cross_val_score(DTregressor, x, y, cv=k_fold , scoring= 'neg_mean_absolute_percentage_error')\n",
        "results3 = cross_val_score(DTregressor, x, y, cv=k_fold , scoring= 'r2')\n"
      ],
      "metadata": {
        "id": "qqcYbZdx5F7z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results1"
      ],
      "metadata": {
        "id": "bDcVBp8c7cCH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results2"
      ],
      "metadata": {
        "id": "ibneBcFW7din"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_MAE_percentage = sum(results2)/len(results2)\n",
        "print(abs(overall_MAE_percentage)*100)"
      ],
      "metadata": {
        "id": "LXZN_E8e7fKj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overallr2 = sum(results3)/len(results3)\n",
        "print(overallr2)"
      ],
      "metadata": {
        "id": "KfE3YZxb8VtN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}