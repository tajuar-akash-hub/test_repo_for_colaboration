{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tajuar-akash-hub/test_repo_for_colaboration/blob/main/Final_notebooks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Problems :\n",
        "1) In the AdaBoost cell during \"Grid Search\" it is taking a huge time. <br>\n",
        "2) All \"Grid search\" cells taking huge time to execute."
      ],
      "metadata": {
        "id": "FQMKv9_3BOXu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApVk94h7N8Qd"
      },
      "outputs": [],
      "source": [
        "!git clone https://github.com/tajuar-akash-hub/Datasets.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4D4_ZBMcbmcO"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "insurance_fraud=pd.read_csv('/content/Datasets/Health Insurance Fraud Claims csv file.csv')\n",
        "\n",
        "insurance_fraud.head(2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5H2u8IPbvy3"
      },
      "outputs": [],
      "source": [
        "insurance_fraud.describe()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9ZrH3wW6byQ_"
      },
      "outputs": [],
      "source": [
        "age_under_18=insurance_fraud[insurance_fraud['PatientAge']< 18]\n",
        "len(age_under_18)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxE_gCFYb0H_"
      },
      "outputs": [],
      "source": [
        "insurance_fraud = insurance_fraud.drop(age_under_18.index)\n",
        "insurance_fraud.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r1C2oUE8ekTn"
      },
      "source": [
        "#remove unwanted columns from the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Dexs1DO_eeXP"
      },
      "outputs": [],
      "source": [
        "insurance_fraud = insurance_fraud[[\"ClaimDate\",\"ClaimAmount\",\"PatientAge\",\"PatientGender\",\"ProviderSpecialty\",\"ClaimStatus\",\"PatientIncome\",\"PatientMaritalStatus\",\"PatientEmploymentStatus\",\"ProviderLocation\",\"ClaimType\",\"ClaimSubmissionMethod\",\"ClaimLegitimacy\"]]\n",
        "\n",
        "print(f\"Shape of the datasets: {insurance_fraud.shape}\")\n",
        "\n",
        "insurance_fraud.head(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IVKvizXqeocm"
      },
      "source": [
        "# all coulmns unique values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KokpJhFkepYb"
      },
      "outputs": [],
      "source": [
        "all_unique=insurance_fraud.nunique()\n",
        "all_unique_df = pd.DataFrame(all_unique, columns=['Unique Value Count'])\n",
        "all_unique_df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZ0BI7aZet7c"
      },
      "source": [
        "# Check duplicated values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "--_LHPtPev_e"
      },
      "outputs": [],
      "source": [
        "duplicates=insurance_fraud.duplicated()\n",
        "flag=0\n",
        "for duplicate in duplicates:\n",
        "  if duplicate == True:\n",
        "    flag=1\n",
        "    print(f\"duplicate values {duplicate}\")\n",
        "if flag == 0:\n",
        "  print(\"No duplicates row\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX1_6z56b-Zf"
      },
      "source": [
        "# Check Outliers with  IQR outliers data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CO-75uH6cLR7"
      },
      "outputs": [],
      "source": [
        "iqr_outlier_data = insurance_fraud.copy()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KNVmm8zCb2t2"
      },
      "outputs": [],
      "source": [
        "def iqr_outliers(data,x):\n",
        "  Q1=np.percentile(data[x],25)\n",
        "  Q3=np.percentile(data[x],75)\n",
        "  IQR=Q3-Q1\n",
        "\n",
        "  lower_bound=Q1 - (1.5 * IQR)\n",
        "  upper_bound=Q1 + (1.5 * IQR)\n",
        "\n",
        "  outliers=[]\n",
        "  outliers_index=[]\n",
        "  for i,value in data[x].items():\n",
        "    if value< lower_bound or value > upper_bound:\n",
        "      outliers.append(value)\n",
        "      outliers_index.append(i)\n",
        "  return outliers,outliers_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UJz3VSLjchHY"
      },
      "outputs": [],
      "source": [
        "## outliers in PatientIncome\n",
        "income,outliers_index=iqr_outliers(iqr_outlier_data,\"PatientIncome\")\n",
        "\n",
        "\n",
        "print(f\"outliers in PatientIncome : {len(income)}\")\n",
        "\n",
        "print(f\"outliers Indexes in PatientIncome : {outliers_index}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LIMiwe7JckdW"
      },
      "outputs": [],
      "source": [
        "outliers_remove_data = iqr_outlier_data.copy()\n",
        "\n",
        "outliers_replaced_data = iqr_outlier_data.copy()  #not using it currently\n",
        "\n",
        "#ques : which on to use ?\n",
        "\n",
        "print(outliers_remove_data.shape)\n",
        "print(outliers_remove_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "23dEVJJ8gWLL"
      },
      "outputs": [],
      "source": [
        "outliers_remove_data = outliers_remove_data.drop(index=outliers_index)\n",
        "\n",
        "#here replace the outlier with mean income\n",
        "\n",
        "\n",
        "print(f\"After removing outliers from 'PatientIncome': {outliers_remove_data.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBbHT0b4AT16"
      },
      "source": [
        "## Replace PatientAge columns with mean_age"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jYfWELZGAVI_"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 1. Detect and replace outliers in 'PatientAge'\n",
        "\n",
        "_,outliers_index=iqr_outliers(outliers_remove_data,\"PatientIncome\")\n",
        "\n",
        "mean_income = np.mean(outliers_remove_data['PatientAge'])\n",
        "\n",
        "mean_age = int(mean_income)\n",
        "\n",
        "# Replace the outliers with the mean in 'PatientAge'\n",
        "\n",
        "for index in outliers_index:\n",
        "    outliers_remove_data.loc[index, 'PatientAge'] = mean_age"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cLEBNDYTfKwN"
      },
      "source": [
        "#Encoding Categorical values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J0V4ZH9leS0X"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import LabelEncoder,OrdinalEncoder\n",
        "le=LabelEncoder()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZPBFv6GfW6j"
      },
      "source": [
        "# Use Ordinal encoders to encode ClaimStatus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6hWYAOYfID3"
      },
      "outputs": [],
      "source": [
        "ordinal_categories=[\"Denied\",\"Pending\",\"Approved\"]\n",
        "\n",
        "ordinal=OrdinalEncoder(categories=[ordinal_categories])\n",
        "\n",
        "outliers_remove_data[\"ClaimStatus\"]=ordinal.fit_transform(outliers_remove_data[[\"ClaimStatus\"]])\n",
        "\n",
        "outliers_remove_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cJASBUGnfZH-"
      },
      "outputs": [],
      "source": [
        "categorical_columns=[\"PatientGender\",\n",
        "                     \"ClaimLegitimacy\",\n",
        "                     \"ProviderSpecialty\",\n",
        "                     \"PatientMaritalStatus\",\n",
        "                     \"PatientEmploymentStatus\",\n",
        "                     \"ProviderLocation\",\n",
        "                     \"ClaimType\",\n",
        "                     \"ClaimSubmissionMethod\"\n",
        "                     ]\n",
        "\n",
        "for col in categorical_columns:\n",
        "  outliers_remove_data[col]=le.fit_transform(outliers_remove_data[col])\n",
        "\n",
        "outliers_remove_data.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hEwejKeSf6vH"
      },
      "outputs": [],
      "source": [
        "outliers_remove_data.info()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9z6ynPjOByLp"
      },
      "source": [
        "# Check ClaimLegitimacy distributions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K6j10K1hf9f-"
      },
      "outputs": [],
      "source": [
        "value_counts = outliers_remove_data.groupby('ClaimLegitimacy').size().reset_index(name = 'Count')\n",
        "\n",
        "value_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QGg-El-jB-AX"
      },
      "source": [
        "# OverSampling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GwzK0opgB6XQ"
      },
      "outputs": [],
      "source": [
        "# Convert 'ClaimDate' to ordinal\n",
        "outliers_remove_data['ClaimDate'] = pd.to_datetime(outliers_remove_data['ClaimDate']).apply(lambda date: date.toordinal())\n",
        "\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "X_smote = outliers_remove_data.drop('ClaimLegitimacy',axis=1)\n",
        "y_smote = outliers_remove_data['ClaimLegitimacy']\n",
        "\n",
        "smote = SMOTE(random_state=42)\n",
        "\n",
        "X_resampled_replace , y_resampled_replace = smote.fit_resample(X_smote,y_smote)\n",
        "\n",
        "outliers_remove_data = pd.DataFrame(X_resampled_replace , columns = X_smote.columns)\n",
        "\n",
        "outliers_remove_data['ClaimLegitimacy'] = y_resampled_replace"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_resampled_replace)"
      ],
      "metadata": {
        "id": "O-x5drAIj8KQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(y_resampled_replace)"
      ],
      "metadata": {
        "id": "pAt5R0pwkADu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(outliers_remove_data)"
      ],
      "metadata": {
        "id": "aqGiHs3NkSIc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tsnYZh_9CSqf"
      },
      "outputs": [],
      "source": [
        "value_counts =outliers_remove_data.groupby('ClaimLegitimacy').size().reset_index(name = 'Count')\n",
        "\n",
        "value_counts"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ME0s4jp7CelY"
      },
      "source": [
        "# Data Spliting (Target and features variables) with Oversampling in outliers_replace_data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Od9fwb9yCW8f"
      },
      "outputs": [],
      "source": [
        "#import libraries\n",
        "\n",
        "from sklearn.model_selection import train_test_split,KFold,cross_val_score\n",
        "\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UsbU4mltCk6p"
      },
      "outputs": [],
      "source": [
        "min_max=MinMaxScaler()\n",
        "\n",
        "X = outliers_remove_data.drop('ClaimLegitimacy',axis=1)\n",
        "y = outliers_remove_data['ClaimLegitimacy']\n",
        "\n",
        "X_smote_scaled_replace = min_max.fit_transform(X)\n",
        "\n",
        "\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(X_smote_scaled_replace,y,test_size=0.2,random_state = 42)\n",
        "\n",
        "\n",
        "# why after scaling X size reduced  (initially it was 6862, now 5489)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_smote_scaled_replace)"
      ],
      "metadata": {
        "id": "uv6_S8ZulR4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train)"
      ],
      "metadata": {
        "id": "K1wOvc3els5P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dVkRRkKCCtjV"
      },
      "outputs": [],
      "source": [
        "X_train.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Start applying ML Models**"
      ],
      "metadata": {
        "id": "8glyioCXEQnx"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzLHbwU0CxD0"
      },
      "source": [
        "# LogisticRegressionClassifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoxquRK6CuGH"
      },
      "outputs": [],
      "source": [
        "# import the class\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# instantiate the model (using the default parameters)\n",
        "logreg_replace = LogisticRegression(random_state=16)\n",
        "\n",
        "# fit the model with data\n",
        "\n",
        "logreg_replace.fit(X_train,y_train)\n",
        "\n",
        "#predict the outputs\n",
        "logreg_ypreds_replace= logreg_replace.predict(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YSYS3siC1kp"
      },
      "outputs": [],
      "source": [
        "# Evaluate model accuracy\n",
        "\n",
        "accuracy_logreg_replace = accuracy_score(y_test, logreg_ypreds_replace)\n",
        "print(f'Accuracy with smote data: {accuracy_logreg_replace * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report with smote data:\")\n",
        "\n",
        "print(classification_report(y_test, logreg_ypreds_replace))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"\\nConfusion Matrix with smote data:\")\n",
        "\n",
        "print(confusion_matrix(y_test, logreg_ypreds_replace))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YSlad0GMC-ph"
      },
      "source": [
        "#  Ada Boost Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dmzvYyZ4C2H3"
      },
      "outputs": [],
      "source": [
        "# Create adaboost classifer object\n",
        "from sklearn.ensemble import AdaBoostClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "#Instanntiate the model\n",
        "replace_adaboost = AdaBoostClassifier()\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 150,200],        # Test different numbers of estimators\n",
        "    'learning_rate': [0.01, 0.1, 0.5, 1],  # Test different learning rates\n",
        "    'estimator': [DecisionTreeClassifier(max_depth=1),  # Shallow tree\n",
        "                       DecisionTreeClassifier(max_depth=3)]  # Slightly deeper tree\n",
        "}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4yudlXtDA_Q"
      },
      "outputs": [],
      "source": [
        "# Create a GridSearchCV object with cross-validation\n",
        "\n",
        "grid_search = GridSearchCV(estimator=replace_adaboost,\n",
        "                           param_grid=param_grid,\n",
        "                           cv=5,  # 5-fold cross-validation\n",
        "                           scoring='accuracy',  # Evaluate using accuracy\n",
        "                           n_jobs=-1)  # Use all available cores\n",
        "\n",
        "# Fit the model with grid search on the training data\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and the corresponding score\n",
        "\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "\n",
        "print(f\"Best Score: {grid_search.best_score_}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DGpgDMIpDMfx"
      },
      "outputs": [],
      "source": [
        "# Use the best model to predict on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred_ada = best_model.predict(X_test)\n",
        "\n",
        "# Calculate the accuracy on the test set\n",
        "test_acc_ada_replace = accuracy_score(y_test, y_pred_ada)\n",
        "\n",
        "# Print the test set accuracy\n",
        "print(f'Accuracy with smote data: {test_acc_ada_replace * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report with smote data:\")\n",
        "print(classification_report(y_test, y_pred_ada))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"\\nConfusion Matrix with smote data:\")\n",
        "print(confusion_matrix(y_test, y_pred_ada))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k07LC2-DDSkQ"
      },
      "outputs": [],
      "source": [
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "plt.figure(figsize=(7,5))\n",
        "sn.heatmap(confusion_matrix(y_test, y_pred_ada), annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t6ySv9kdDXpt"
      },
      "source": [
        "# **ExraTreeClassifiers**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NcE0A7fHDTGI"
      },
      "outputs": [],
      "source": [
        "\n",
        "from sklearn.ensemble import ExtraTreesClassifier\n",
        "\n",
        "# Define the ExtraTreesClassifier\n",
        "extra_tree_replace = ExtraTreesClassifier(random_state=42)\n",
        "\n",
        "# Define a parameter grid for tuning\n",
        "\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [None, 10, 20, 30],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4]\n",
        "}\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X2PKmcKxDWHj"
      },
      "outputs": [],
      "source": [
        "extraclassi_model=ExtraTreesClassifier()\n",
        "\n",
        "extraclassi_model.fit(X_train,y_train)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AXGHabwFDyMw"
      },
      "outputs": [],
      "source": [
        "importance_features=pd.Series(extraclassi_model.feature_importances_,index=X.columns)\n",
        "importance_features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WERSiJ3pD7lg"
      },
      "outputs": [],
      "source": [
        "importance_features.nlargest(10).plot(kind='bar',color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b', '#e377c2', '#7f7f7f', '#bcbd22', '#17becf'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ARhhWzhLEHJg"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Create GridSearchCV object\n",
        "grid_search = GridSearchCV(estimator=extra_tree_replace, param_grid=param_grid, cv=5, scoring='accuracy', n_jobs=-1)\n",
        "\n",
        "# Fit the model with grid search on the training data\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Print the best parameters and the best score from the grid search\n",
        "print(f\"Best Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Best Cross-Validation Accuracy: {grid_search.best_score_}\")\n",
        "\n",
        "# Use the best model to predict on the test set\n",
        "best_model = grid_search.best_estimator_\n",
        "y_pred_extra = best_model.predict(X_test)\n",
        "\n",
        "# Calculate accuracy on the test set\n",
        "test_acc_extratree_replace = accuracy_score(y_test,y_pred_extra)\n",
        "print(f\"Test Accuracy: {test_acc_extratree_replace}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hiPXJrdCEc6f"
      },
      "source": [
        "# Voting Classifiers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gBLH7-yZEbIy"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.ensemble import VotingClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "\n",
        "\n",
        "log_clf = LogisticRegression(random_state=42)\n",
        "rf_clf = RandomForestClassifier(random_state=42)\n",
        "svm_clf = SVC(probability=True, random_state=42)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EUjtGHgTEl-7"
      },
      "source": [
        "# Soft"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZZCbXl-Ef_G"
      },
      "outputs": [],
      "source": [
        "# Create a Voting Classifier with soft voting\n",
        "voting_clf = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rf_clf), ('svc', svm_clf)],\n",
        "    voting='soft'  # Use 'hard' for majority voting\n",
        ")\n",
        "\n",
        "# Train the Voting Classifier\n",
        "voting_clf.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = voting_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f\"Voting Classifier Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "# Evaluate individual models\n",
        "for clf in (log_clf, rf_clf, svm_clf, voting_clf):\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred_individual = clf.predict(X_test)\n",
        "    print(f\"{clf.__class__.__name__} Accuracy: {accuracy_score(y_test, y_pred_individual):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wi7LfqqBEptB"
      },
      "source": [
        "# Hard"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pJvST7wQEift"
      },
      "outputs": [],
      "source": [
        "# Create a Voting Classifier with soft voting\n",
        "\n",
        "voting_clf_hard = VotingClassifier(\n",
        "    estimators=[('lr', log_clf), ('rf', rf_clf), ('svc', svm_clf)],\n",
        "    voting='hard'  # Use 'hard' for majority voting\n",
        ")\n",
        "\n",
        "# Train the Voting Classifier\n",
        "voting_clf_hard.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred_hard = voting_clf.predict(X_test)\n",
        "\n",
        "# Calculate accuracy\n",
        "accuracy_hard = accuracy_score(y_test, y_pred_hard)\n",
        "print(f\"Voting Classifier Accuracy: {accuracy_hard:.4f}\")\n",
        "\n",
        "# Evaluate individual models\n",
        "for clf in (log_clf, rf_clf, svm_clf, voting_clf):\n",
        "    clf.fit(X_train, y_train)\n",
        "    y_pred_individual_hard = clf.predict(X_test)\n",
        "    print(f\"{clf.__class__.__name__} Accuracy: {accuracy_score(y_test, y_pred_individual_hard):.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RMaDl3XSIPuK"
      },
      "source": [
        "# SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHkI4pzZIOsv"
      },
      "outputs": [],
      "source": [
        "from sklearn.svm import SVC\n",
        "svc = SVC()\n",
        "\n",
        "# Train the model\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svc.predict(X_test)\n",
        "\n",
        "# Evaluate model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "# Print confusion matrix\n",
        "\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "plt.figure(figsize=(5,5))\n",
        "sn.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XSWpzdPeJPB4"
      },
      "outputs": [],
      "source": [
        "X_train"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random forest"
      ],
      "metadata": {
        "id": "wq4ncdYIEg_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "random_model = RandomForestClassifier(random_state=42)\n",
        "\n",
        "# random_model.fit(X_train,y_train)\n",
        "\n",
        "# Train the model\n",
        "random_model.fit(X_train , y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = random_model.predict(X_test)\n",
        "\n",
        "# Evaluate model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "\n",
        "%matplotlib inline\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "plt.figure(figsize=(7,5))\n",
        "sn.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')\n",
        "\n",
        "\n",
        "\n",
        "#Cross Validation\n",
        "\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "scores = cross_val_score(random_model, X_train, y_train, cv=5)    #By Akash (experimental)\n",
        "\n",
        "\n",
        "print(f'Cross-validation scores: {scores}')\n",
        "print(f'Mean accuracy: {scores.mean() * 100:.2f}%')"
      ],
      "metadata": {
        "id": "ZnmQL3UiEhO-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Support vector Classifier"
      ],
      "metadata": {
        "id": "gmvjFnBzE_hu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.svm import SVC\n",
        "\n",
        "svc = SVC()\n",
        "\n",
        "# Train the model\n",
        "svc.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = svc.predict(X_test)\n",
        "\n",
        "# Evaluate model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "plt.figure(figsize=(5,5))\n",
        "sn.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')"
      ],
      "metadata": {
        "id": "r09pM5j1FOoT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# KNN"
      ],
      "metadata": {
        "id": "cTKduB-NFSct"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier(n_neighbors=5)\n",
        "\n",
        "# Train the model\n",
        "knn.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = knn.predict(X_test)\n",
        "\n",
        "# Evaluate model accuracy\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy * 100:.2f}%')                                        #do we need to Scale our data for KNN ( by Akash) ?\n",
        "\n",
        "# Print classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "# Print confusion matrix\n",
        "print(\"\\nConfusion Matrix:\")\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sn\n",
        "plt.figure(figsize=(5,5))\n",
        "sn.heatmap(confusion_matrix(y_test, y_pred), annot=True, fmt='g')\n",
        "plt.xlabel('Predicted')\n",
        "plt.ylabel('Truth')"
      ],
      "metadata": {
        "id": "A2X-VRtaFZ_m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logistic Regression ( BY Akash)"
      ],
      "metadata": {
        "id": "Dg2b_SEIFd9l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "model = LogisticRegression(penalty = 'l2' , tol = 0.0001 , solver = 'liblinear',\n",
        "                           max_iter=100,multi_class='auto')\n",
        "\n",
        "#tol = tolerance , max_itr = max iteration\n",
        "\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "pred_values = model.predict(X_test)\n",
        "\n",
        "\n",
        "model.fit(X_train,y_train)\n",
        "\n",
        "\n",
        "pred_values = model.predict(X_test)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "# Get predictions (class labels)\n",
        "pred_values = model.predict(X_test)\n",
        "\n",
        "# Compute the confusion matrix\n",
        "cm = confusion_matrix(y_test, pred_values)\n",
        "\n",
        "# Print the confusion matrix\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "\n",
        "\n",
        "#Evaluation metrices\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.metrics import precision_score\n",
        "from sklearn.metrics import recall_score\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import roc_curve\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "\n",
        "accuracy = accuracy_score(y_test,pred_values)\n",
        "\n",
        "precision = precision_score(y_test,pred_values)\n",
        "\n",
        "recall = recall_score(y_test,pred_values)\n",
        "\n",
        "f1 = f1_score(y_test,pred_values)\n",
        "\n",
        "AUC = roc_auc_score(y_test,pred_values)\n",
        "\n",
        "\n",
        "print(\"accuracy : \",accuracy*100)\n",
        "print(\"precision : \",precision*100)\n",
        "print(\"recall : \",recall*100)\n",
        "print(\"f1 score : \",f1*100)\n",
        "print(\"AUC Score : \",AUC*100)\n",
        "\n",
        "\n",
        "\n",
        "#Roc curve\n",
        "\n",
        "\n",
        "# Get predicted probabilities\n",
        "y_pred_prob = model.predict_proba(X_test)[:, 1]  # Probability for the positive class (1)\n",
        "\n",
        "# Calculate fpr and tpr for the ROC curve\n",
        "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
        "\n",
        "# Plot the ROC curve\n",
        "plt.plot(fpr, tpr)\n",
        "plt.plot([0, 1], ls=\"--\")  # Random guess line\n",
        "\n",
        "plt.title(\"ROC Curve for Logistic Regression\")\n",
        "plt.ylabel(\"True Positive Rate\")\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "nZVpgJ0UFiqd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Naive Bayes Classifier (By Akash)"
      ],
      "metadata": {
        "id": "Kd3QGtkrF-Am"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "#Standard Scaler for Naive Bayes\n",
        "\n",
        "STD_Scaler = StandardScaler()\n",
        "STD_Scaled_X =  STD_Scaler.fit_transform(X)\n",
        "STD_Scaled_X\n",
        "\n",
        "\n",
        "#train-test split\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "x_train,x_test,y_train,y_test = train_test_split(STD_Scaled_X,y,test_size=0.20)\n",
        "\n",
        "print(\"train data size (features):\",len(x_train))\n",
        "print(\"train data size (target):\",len(y_train))\n",
        "\n",
        "print(\"test data size(feature)\",len(x_test))\n",
        "print(\"test data size(target)\",len(y_test))\n",
        "\n",
        "\n",
        "len(X_train)"
      ],
      "metadata": {
        "id": "8tugoRLgF_sO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#K-fold validation\n",
        "\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "\n",
        "NB_Classifier = GaussianNB(priors= None , var_smoothing =  1e-09 )    #1e-09 = 1*10^-9\n",
        "\n",
        "\n",
        "#we add very small value in var_smoothing to avoid 0/0 form\n",
        "\n",
        "# priors = prior probability  (see the documentation from sklearn)\n",
        "\n",
        "# var_smoothing = variance smothing where variance = sigma square\n",
        "\n"
      ],
      "metadata": {
        "id": "tHPVbZLCHFj-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#K fold for hyper parameter tuning\n",
        "\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "K_fold = KFold(10)\n",
        "\n",
        "accuracy = cross_val_score(NB_Classifier , X_train,y_train, cv =K_fold , scoring = 'accuracy' )\n",
        "\n",
        "precision = cross_val_score(NB_Classifier , X_train,y_train, cv =K_fold , scoring = 'precision' )\n",
        "\n",
        "recall = cross_val_score(NB_Classifier , X_train,y_train, cv =K_fold , scoring = 'recall' )\n",
        "\n",
        "f1_score = cross_val_score(NB_Classifier , X_train,y_train, cv =K_fold , scoring = 'f1' )\n",
        "\n",
        "AUC = cross_val_score(NB_Classifier , X_train,y_train, cv =K_fold , scoring = 'roc_auc' )\n",
        "\n"
      ],
      "metadata": {
        "id": "coPotmm5HBYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "accuracy\n",
        "overall_accuracy = sum(accuracy) / len(accuracy)\n",
        "print(f'Overall Accuracy = {overall_accuracy*100:0.2f}%')\n",
        "\n",
        "\n",
        "recall\n",
        "overall_recall = sum(recall) / len(recall)\n",
        "print(f'overall_recall = {overall_recall*100:0.2f}%')\n",
        "\n",
        "\n",
        "f1_score\n",
        "\n",
        "overall_f1_score = sum(f1_score) / len(f1_score)\n",
        "print(f'overall_f1_score = {overall_f1_score*100:0.2f}%')\n",
        "\n",
        "\n",
        "overall_AUC = sum(AUC) / len(AUC)\n",
        "print(f'overall_AUC = {overall_AUC*100:0.2f}%')"
      ],
      "metadata": {
        "id": "8x3VmYO-G_D3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Applying the Algorithm after k-fold\n",
        "\n",
        "NB_Classifier = GaussianNB(priors= None , var_smoothing =  1e-09 )  #tune var_smoothing\n",
        "NB_Classifier.fit(X_train,y_train)\n",
        "\n",
        "#unseen data\n",
        "\n",
        "unseen_prediction = NB_Classifier.predict(X_test)"
      ],
      "metadata": {
        "id": "xjdlWvJLG8xv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Evaluation metrices\n",
        "\n",
        "from sklearn.metrics import accuracy_score, precision_score,recall_score,f1_score ,roc_curve ,roc_auc_score\n",
        "\n",
        "accuracy = accuracy_score(unseen_prediction,y_test)\n",
        "\n",
        "precision = precision_score(unseen_prediction,y_test)\n",
        "\n",
        "recall = recall_score(unseen_prediction,y_test)\n",
        "\n",
        "f1 = f1_score(unseen_prediction,y_test)\n",
        "\n",
        "AUC = roc_auc_score(unseen_prediction,y_test)\n",
        "\n",
        "print(f'accuracy = {accuracy*100:0.2f}%')\n",
        "print(f'precision = {precision*100:0.2f}%')\n",
        "print(f'recall = {recall*100:0.2f}%')\n",
        "print(f'f1 = {f1*100:0.2f}%')"
      ],
      "metadata": {
        "id": "k0QObGmcG4wx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Decision Tree Classifier (Akash)"
      ],
      "metadata": {
        "id": "OXae7f3kGh3F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "DTregressor = DecisionTreeRegressor(criterion='squared_error',\n",
        "                                  max_depth= 10,\n",
        "                                  min_samples_split= 2,\n",
        "                                  min_samples_leaf= 1,\n",
        "                                  max_features= None,\n",
        "                                  max_leaf_nodes= None,\n",
        "                                  min_impurity_decrease= 0.01\n",
        "                                  )"
      ],
      "metadata": {
        "id": "bLdlo9rcGiJF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "from sklearn.model_selection import KFold\n",
        "\n",
        "k_fold = KFold(10)\n",
        "\n",
        "results1 = cross_val_score(DTregressor, X_train, y_train, cv=k_fold , scoring='neg_mean_squared_error')\n",
        "results2 = cross_val_score(DTregressor, X_train, y_train, cv=k_fold , scoring= 'neg_mean_absolute_percentage_error')\n",
        "results3 = cross_val_score(DTregressor, X_train, y_train, cv=k_fold , scoring= 'r2')"
      ],
      "metadata": {
        "id": "D38CLQ_SGnKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results1"
      ],
      "metadata": {
        "id": "tDqcNKdcGpBM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results2"
      ],
      "metadata": {
        "id": "z4me4GegGqVu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overall_MAE_percentage = sum(results2)/len(results2)\n",
        "\n",
        "print(abs(overall_MAE_percentage)*100)"
      ],
      "metadata": {
        "id": "yajSNFinGrYN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overallr2 = sum(results3)/len(results3)\n",
        "print(overallr2)"
      ],
      "metadata": {
        "id": "54WYoH5UGrau"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"akash \")"
      ],
      "metadata": {
        "id": "LXa5aBHYGrdN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "sdMKTqF9F1FO"
      }
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}